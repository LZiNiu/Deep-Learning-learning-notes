{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.datasets import load_iris\n",
    "import warnings\n",
    "from typing import Union\n",
    "import time\n",
    "import seaborn\n",
    "from torch import nn\n",
    "from typing import Optional\n",
    "from torchvision import transforms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "\n",
    "train_data = datasets.MNIST(root=\"./DataSets/\",train=True, transform=transforms.ToTensor(),download=False)\n",
    "train_loader = DataLoader(train_data,batch_size=64, shuffle=True)\n",
    "test_data = datasets.MNIST(root=\"./DataSets/\",train=False, transform=transforms.ToTensor(),download=False)\n",
    "test_loader = DataLoader(test_data)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建一个简单的CNN网络"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用tenor手动实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyConv2d(object):\n",
    "    def __init__(self,\n",
    "        in_channels:int, \n",
    "        out_channels:int,\n",
    "        kernel_size:torch.Size=(3,3),\n",
    "        padding:Union[int,Optional[torch.Size],None]=1,\n",
    "        stride:Union[int,Optional[torch.Size]]=1) -> None:\n",
    "        \n",
    "        self.in_channels = in_channels # 输入通道数\n",
    "        self.out_channels = out_channels # 输出通道数\n",
    "        self.padding = padding # 填充size\n",
    "        self.stride = stride # 步长\n",
    "        self.kernel_size = kernel_size # 卷积核size\n",
    "        # 权重在卷积层中即为卷积核\n",
    "        self.weight = torch.rand((self.out_channels, self.in_channels, self.kernel_size[0], self.kernel_size[1]))\n",
    "        self.bias = torch.zeros(1)\n",
    "        \n",
    "        \n",
    "    def zero_padding(self,X:torch.Tensor)->torch.Tensor:\n",
    "        \"\"\"零填充函数\n",
    "\n",
    "        Args:\n",
    "            X (torch.Tensor): (height, width)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: (height+2, width+2)\n",
    "        \"\"\"        \n",
    "        # 不作padding处理,将数据直接返回\n",
    "        if self.padding is None or self.padding==0:\n",
    "            return X\n",
    "        if type(self.padding) is int:\n",
    "            expand_h = 2 * self.padding\n",
    "            expand_w = expand_h\n",
    "            # 初始化一个目标形状的tensor来存放填充操作的新tensor,返回值不会修改原数据,但是可以抛弃原数据的引用\n",
    "            temp = torch.zeros(X.shape[0], X.shape[1], X.shape[2] + expand_h, X.shape[3] + expand_w)\n",
    "            for i in range(X.shape[0]):\n",
    "                for j in range(X.shape[1]):\n",
    "                    temp[i, j, self.padding : -self.padding, self.padding : -self.padding] = X[i, j, :, :]\n",
    "        else:\n",
    "            # 如果没有padding,将数据直接返回\n",
    "            if tuple(self.padding)==(0,0):\n",
    "                return X\n",
    "            expand_h = 2 * self.padding[0]\n",
    "            expand_w = 2 * self.padding[1]\n",
    "            # 初始化一个目标形状的tensor来存放填充操作的新tensor,返回值不会修改原数据,但是可以抛弃原数据的引用\n",
    "            temp = torch.zeros(X.shape[0], X.shape[1], X.shape[2] + expand_h, X.shape[3] + expand_w)\n",
    "            if self.padding[0]==0:\n",
    "                #padding(0, w)\n",
    "                for i in range(X.shape[0]):\n",
    "                    for j in range(X.shape[1]):\n",
    "                        temp[i, j, : , self.padding[1] : -self.padding[1]] = X[i, j, :, :]\n",
    "            else:\n",
    "                # padding(h, 0)\n",
    "                if self.padding[1]==0:\n",
    "                    for i in range(X.shape[0]):\n",
    "                        for j in range(X.shape[1]):\n",
    "                            temp[i, j, self.padding[0] : -self.padding[0], : ] = X[i, j, :, :]\n",
    "                \n",
    "                else:\n",
    "                    # padding(h, w)都不为0\n",
    "                    for i in range(X.shape[0]):\n",
    "                        for j in range(X.shape[1]):\n",
    "                            temp[i, j, self.padding[0] : -self.padding[0], self.padding[1] : -self.padding[1]] = X[i, j, :, :]\n",
    "        \n",
    "        return temp\n",
    "\n",
    "    def corr2d(self, X:torch.Tensor, kernel:torch.Tensor)->torch.Tensor:\n",
    "        \"\"\"单通道单个卷积核卷积函数\n",
    "\n",
    "        Args:\n",
    "            X (torch.Tensor): 数据的某个通道\n",
    "            kernel (torch.Tensor): 二维卷积核\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: 卷积结果矩阵\n",
    "        \"\"\" \n",
    "        \n",
    "        h, w = kernel.shape\n",
    "        # 利用公式计算目标结果的形状开始给对应位置卷积计算赋值\n",
    "        if type(self.stride) is int:\n",
    "            Y = torch.zeros(int((X.shape[0]-h)/self.stride) + 1, int((X.shape[1]-w)/self.stride) + 1)\n",
    "            for i in range(0,Y.shape[0]):\n",
    "                for j in range(Y.shape[1]):\n",
    "                    Y[i,j] = (X[i + self.stride-1 : i + self.stride-1 + h, j + self.stride-1 : j + self.stride-1 + w] * kernel).sum()\n",
    "        else:\n",
    "            Y = torch.zeros(int((X.shape[0]-h)/self.stride[0]) + 1, int((X.shape[1]-w)/self.stride[1]) + 1)\n",
    "            for i in range(0,Y.shape[0]):\n",
    "                for j in range(Y.shape[1]):\n",
    "                    Y[i,j] = (X[i + self.stride[0]-1 : i + self.stride[0]-1 + h, j + self.stride[1]-1 : j + self.stride[1]-1 + w] * kernel).sum()\n",
    "            \n",
    "        return Y\n",
    "\n",
    "    def corr2d_multi_in(self, X:torch.Tensor, kernels:torch.Tensor)->torch.Tensor:\n",
    "        \"\"\"多通道输入卷积计算函数\n",
    "\n",
    "        Args:\n",
    "            X (torch.Tensor): 具有多个通道的数据\n",
    "            Kernel (torch.Tensor): 卷积核的tensor集合,分别对应每个通道\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: 多个通道卷积结果的累加\n",
    "        \"\"\"\n",
    "        return sum(self.corr2d(x, k) for x,k in zip(X, kernels))\n",
    "\n",
    "    def corr2d_multi_in_out(self,X:torch.Tensor, Kernels: torch.Tensor)->torch.Tensor:\n",
    "        return torch.stack([self.corr2d_multi_in(X,kernels) for kernels in Kernels], dim=0)\n",
    "\n",
    "    def forward(self, X:torch.Tensor)->torch.Tensor:\n",
    "        \"\"\"前向传播\n",
    "\n",
    "        Args:\n",
    "            X (torch.Tensor): with size (batch_size, channel,height,width)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: with size (batch_size, out_channels, res_h, res_w)\n",
    "        \"\"\"        \n",
    "        # 对数据进行padding处理\n",
    "        X = self.zero_padding(X)\n",
    "        \n",
    "        res = [self.corr2d_multi_in_out(x, self.weight) + self.bias for x in X]\n",
    "        # 将tensor list 转换成tensor的同时keepdim,若采用cat将丢失维度\n",
    "        return torch.stack(res, dim=0)\n",
    "\n",
    "class MyMaxPooling2d(object):\n",
    "    def __init__(self, kernel_size:Optional[list]=(2,2), stride:Union[int,Optional[list]]=2) -> None:\n",
    "        # 仍然有padding和stride的参数选项,但此处不在增加\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, X:torch.Tensor)->torch.Tensor:\n",
    "        \"\"\"前向传播\n",
    "\n",
    "        Args:\n",
    "            X (torch.Tensor): with size (batch_size, channel, height, width)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: with the same dimension of X\n",
    "        \"\"\"\n",
    "        \n",
    "        h, w = self.kernel_size\n",
    "        if type(self.stride) is int:\n",
    "            Y = torch.zeros(X.shape[0],X.shape[1], int((X.shape[2]-h)/self.stride) + 1, int((X.shape[3]-w)/self.stride) + 1)\n",
    "            for i in range(Y.shape[0]):\n",
    "                for j in range(Y.shape[1]):\n",
    "                    for k in range(Y.shape[2]):\n",
    "                        for m in range(Y.shape[3]):\n",
    "                            Y[i,j,k,m] = X[i,j, k:k+h,m:m+w].max()\n",
    "        else:\n",
    "            Y = torch.zeros(X.shape[0],X.shape[1], int((X.shape[2]-h)/self.stride[0]) + 1, int((X.shape[3]-w)/self.stride[1]) + 1)\n",
    "            for i in range(Y.shape[0]):\n",
    "                for j in range(Y.shape[1]):\n",
    "                    for k in range(Y.shape[2]):\n",
    "                        for m in range(Y.shape[3]):\n",
    "                            Y[i,j,k,m] = X[i,j, k+self.stride[0]-1:k+h+self.stride[0]-1,m+self.stride[1]-1:m+w+self.stride[1]-1].max()\n",
    "        return Y\n",
    "\n",
    "class MyCNN(object):\n",
    "    def __init__(self, in_channels:int, n_classes:int) -> None:\n",
    "        self.l0 = MyConv2d(in_channels=in_channels, out_channels=4, kernel_size=(3,3))\n",
    "        self.act = nn.ReLU()\n",
    "        self.l1 = MyMaxPooling2d(kernel_size=(2,2),stride=(2,2))\n",
    "        self.l2 = MyConv2d(in_channels=4,out_channels=6,kernel_size=(3,3))\n",
    "        self.l3 = nn.Linear(6*14*14,out_features=n_classes)\n",
    "    def forward(self,X:torch.Tensor)->torch.Tensor:\n",
    "        x = self.l0.forward(X)\n",
    "        x = self.act(x)\n",
    "        x = self.l1.forward(x)\n",
    "        x = self.l2.forward(x)\n",
    "        x = self.act(x)\n",
    "        # 拉直\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        \n",
    "        x = self.l3(x)\n",
    "        return x\n",
    "    \n",
    "    def loss(self, y:torch.Tensor, y_hat:torch.Tensor)->torch.Tensor:\n",
    "        fn = nn.functional.cross_entropy(y,y_hat)\n",
    "\n",
    "    def configure_optimizer(self, lr:float)->Optional[torch.optim.SGD]:\n",
    "        pass\n",
    "\n",
    "    def fit(self, loader:DataLoader, epoches=5, lr=0.001):\n",
    "\n",
    "        optimizer = self.configure_optimizer(lr)\n",
    "        for epoch in range(epoches):\n",
    "            for X, y in loader:\n",
    "                optimizer.zero_grad()\n",
    "                l = self.loss(y, self.forward(X))\n",
    "                l.backward()\n",
    "                optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand((64,1,28,28))\n",
    "test = MyConv2d(in_channels=1, out_channels=10 , kernel_size=(3,3), padding=(1,1), stride=(1,1))\n",
    "temp = test.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试前向传播成功,调用自动微分实现反向传播,that should work\n",
    "# 速度很慢都是for写的,也许用c++写好点\n",
    "test1 = MyCNN(in_channels=1,n_classes=10)\n",
    "test1.forward(X).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用Pytorch高级API实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, in_channels:int, n_classes:int) -> None:\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels,out_channels=8,kernel_size=(3,3),stride=(1,1),padding=(1,1)),nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2,2), stride=(2,2)),\n",
    "            nn.Conv2d(in_channels=8,out_channels=16, kernel_size=(3,3),stride=(1,1),padding=(1,1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(16*14*14,out_features=n_classes)\n",
    "        )\n",
    "    def forward(self,X:torch.Tensor)->torch.Tensor:\n",
    "        return self.net(X)\n",
    "    \n",
    "    def configure_optimizer(self,lr):\n",
    "        return torch.optim.Adam(self.net.parameters(), lr)\n",
    "    def loss(self, y, y_hat):\n",
    "        \n",
    "        return nn.functional.cross_entropy(y_hat, y)\n",
    "\n",
    "    def fit(self,device, loader:DataLoader, lr=0.001, epochs=10,):\n",
    "        optimizer = self.configure_optimizer(lr)\n",
    "        for epoch in range(epochs):\n",
    "            for X, y in loader:\n",
    "                # Send the data to cuda to calculate\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                l = self.loss(y, self.net(X))\n",
    "                l.backward()\n",
    "                optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = CNN(1, 10).to(device=device)\n",
    "\n",
    "model.fit(device, train_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([0.9801, 0.9700, 0.9438]),\n",
       "indices=tensor([3, 7, 2]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.rand((3,9)), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.9831\n"
     ]
    }
   ],
   "source": [
    "# 测试集正确率\n",
    "count=0\n",
    "for X,y in test_loader:\n",
    "    X = X.to(device)\n",
    "    y=  y.to(device)\n",
    "    _, prediction = torch.max(model(X), dim=1)\n",
    "    if prediction==y:\n",
    "        count+=1\n",
    "print(\"accuracy: \", count/len(test_loader))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 建立Batch Normalization Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 28, 28])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def try_gpu():\n",
    "    return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "class MyBatchNorm(nn.Module):\n",
    "    def __init__(self, \n",
    "                gamma:Union[float, torch.Tensor]=None, \n",
    "                beta:Union[float,torch.Tensor]=None,\n",
    "                moving_avg:torch.Tensor=None,\n",
    "                moving_var:torch.Tensor=None,\n",
    "                eps:float=1e-5) -> None:\n",
    "        super(MyBatchNorm, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.beta = beta\n",
    "        self.eps = eps\n",
    "        self.moving_avg = moving_avg\n",
    "        self.moving_var = moving_var\n",
    "        \n",
    "        \n",
    "        \n",
    "    def batch_norm(self, X:torch.Tensor, moving_avg:torch.Tensor, moving_var:torch.Tensor, monetum:float, eps:float, mode:str='conv'):\n",
    "        # 如果在训练模式那么需要接收上一轮训练的均值和方差偏移\n",
    "        if not torch.is_grad_enabled():\n",
    "            # 预测模式使用偏移均值和方差来规范化\n",
    "            X_hat = (X - moving_avg)/torch.sqrt(moving_var + eps)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # 前接卷积运算\n",
    "            if mode == 'conv':\n",
    "                # 计算通道维上的均值、方差\n",
    "                avg = X.mean(dim=(0,2,3), keepdim=True)\n",
    "                var = ((X-avg)**2).mean(dim=(0,2,3), keepdim=True)\n",
    "            else:\n",
    "                avg = X.mean(dim=0)\n",
    "                var = ((X - avg)**2).mean(dim=0)\n",
    "        \n",
    "            X_hat =(X-avg)/torch.sqrt(var + eps)\n",
    "            # 设定均值和方差偏移(用于测试集的规范化,模拟训练集和测试集的分布差异)\n",
    "            moving_avg = moving_avg * monetum + (1-monetum) * avg\n",
    "            moving_var = moving_var * monetum + (1-monetum) * var\n",
    "        Y = self.gamma * X_hat + self.beta\n",
    "        return Y, moving_avg, moving_var\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,X:torch.Tensor)->torch.Tensor:\n",
    "        \"\"\"forward\n",
    "\n",
    "        Args:\n",
    "            X (torch.Tensor):要做batchnormalization的数据:(example, chinnel, height, width)型和(example, n_features)型\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor\n",
    "        \"\"\"        \n",
    "        if self.gamma is None:\n",
    "            self.gamma = torch.rand(X.shape)\n",
    "        if self.beta is None:\n",
    "            self.beta = torch.rand(X.shape)\n",
    "        # 检查输入数据的维度(确认前接卷积层还是全连接层)\n",
    "        assert len(X.shape) in (2,4)\n",
    "        n_features = X.shape[1]\n",
    "        if X.dim()==4:\n",
    "            mode = 'conv'\n",
    "            if self.moving_avg is None:\n",
    "                self.moving_avg = torch.zeros(1,n_features, 1, 1)\n",
    "            if self.moving_var is None:\n",
    "                self.moving_var = torch.ones(1, n_features, 1, 1)\n",
    "        else:\n",
    "            if self.moving_avg is None:\n",
    "                self.moving_avg = torch.zeros(1,n_features)\n",
    "            if self.moving_var is None:\n",
    "                self.moving_var = torch.ones(1, n_features)\n",
    "            mode='linear'\n",
    "        # 模型加载到显存上训练将数据全部移至显存上计算\n",
    "        if X.device != self.moving_avg.device:\n",
    "            X = X.to(try_gpu())\n",
    "        Y, self.moving_avg, self.moving_var = self.batch_norm(X, monetum=0.9, moving_avg=self.moving_avg, \n",
    "                                                            moving_var=self.moving_var,eps=self.eps, mode=mode)\n",
    "        return Y\n",
    "\n",
    "\n",
    "model = MyBatchNorm().to(try_gpu())\n",
    "model(torch.rand(64,3,28,28)).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 在LeNet网络中添加自己的BacthNormalization层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Conv2d(1, 6, kernel_size=5), MyBatchNorm(), nn.Sigmoid(),\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "    nn.Conv2d(6, 16, kernel_size=5), MyBatchNorm(), nn.Sigmoid(),\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2), nn.Flatten(),\n",
    "    nn.Linear(16*4*4, 120), MyBatchNorm(), nn.Sigmoid(),\n",
    "    nn.Linear(120, 84), MyBatchNorm(), nn.Sigmoid(),\n",
    "    nn.Linear(84, 10))\n",
    "net(torch.rand(1,1,28,28)).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ebd108c06cc1fee3274f6da06eb87bbf2b402824496318ac7157a648df7690c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
