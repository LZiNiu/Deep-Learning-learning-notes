# 随记

## 1、模型中的一些操作

### 1.1 Softmax的数值计算问题

#### 1.1.1 典型的softmax代码

```python
def softmax(X):
    X_exp = torch.exp(X)
    partition = X_exp.sum(1, keepdims=True)
    return X_exp / partition
```

其虽然直观便于公式理解，但是对于数值不稳定没有足够的健壮性（robust），不建议在正式项目中这么写。



我们计算模型的输出并且使用了交叉熵损失。由于指数运算造成的数值向下溢出和向上溢出，这在数学上时完全合理的。通过softmax的函数表达形式$ \hat y_j = \frac{\exp o_j}{\sum_k \exp o_k} $，其中$k$表示有类别数，$j$则表示第$j$类，$o_j$表示模型在第$j$类上的输出，如果$o_j$很大且是正数，那么$exp(o_j)$将会是一个十分大的数造成数值的`overflow`，反之$o_j$很小（负数）那么将会造成数值的`underflow`，例如单精度浮点数大概范围为$10^{-38}$到$10^{38}$，因此，如果$o$的最大项位于区间$[-90,90]$外，那么计算结果将变得数值不稳定，一个解决办法如下：

定义：$\bar{o} = \stackrel{k}\max o_k$，上述softmax公式变换如下：
$$
\hat y_j = \frac{\exp o_j}{\sum_k \exp o_k} =
\frac{\exp(o_j - \bar{o}) \exp \bar{o}}{\sum_k \exp (o_k - \bar{o}) \exp \bar{o}} =
\frac{\exp(o_j - \bar{o})}{\sum_k \exp (o_k - \bar{o})}.
$$
显然，对于一个$q$分类问题，上式分母的数值必然被限定在区间$[1,q)$，而分子必定小于等于$1$，这解决了数值的overflow问题，但是，underflow问题依然存在，其出现时$exp(o_j-\bar o)$与$0$无异，在我们计算交叉熵$log\hat y_j$时将会导致计算$log0$，在反向传播时，我们将可能遇见许多NaN（Not a Number)的结果。

万幸的是，通过交叉熵和softmax二者结合能发现我们可以避开数值稳定性问题：

计算交叉熵损失时我们将会对softmax的结果进行取$log$的操作：
$$
\log \hat{y}_j =
\log \frac{\exp(o_j - \bar{o})}{\sum_k \exp (o_k - \bar{o})} =
o_j - \bar{o} - \log \sum_k \exp (o_k - \bar{o}).
$$

* 一点疑问：$log exp(o_j-\bar o)==o_j-\bar o???$



### 1.2 神经网络Dropout(暂退法)

克里斯托弗·毕晓普证明了具有输入噪声的训练等价于Tikhonov正则化。这项工作用数学证实了“要求函数光滑”和“要求函数对输入的随机噪声具有适应性”之间的联系。

* 经典泛化理论认为，为了缩小训练和测试性能之间的差距，应该以简单的模型为目标。参数的范数代表了一种有用的简单性度量。

* 另一个角度是平滑性，平滑性，即函数不应该对其输入的微小变化敏感。

在训练过程中，在计算后续层之前向网络的每一层注入噪声。 因为当训练一个有多层的深层网络时，注入噪声只会在输入-输出映射上增强平滑性。

#### 1.2.1 理论操作

`Dropout`在前向传播过程中，计算每一内部层的同时注入噪声。从表面上看是在训练过程中丢弃（drop out）一些神经元。 在整个训练过程的每一次迭代中，标准`Dropout`包括在计算下一层之前将当前层中的一些节点置零。

在标准`Dropout`正则化中，通过按保留（未丢弃）的节点的分数进行规范化来消除每一层的偏差。 换言之，每个中间活性值 $h$ 以暂退概率 $p$ 由随机变量 $h'$ 替换，如下所示：
$$
\begin{split}\begin{aligned}
h' =
\begin{cases}
    0 & \text{ 概率为 } p \\
    \frac{h}{1-p} & \text{ 其他情况}
\end{cases}
\end{aligned}\end{split}
$$
由以上公式易得$E(h')=h$，即期望仍保持不变。

#### 1.2.2 简单的单层Dropout代码实现

```python
def dropout_layer(X, dropout):
    assert 0 <= dropout <= 1
    # 在本情况中，所有元素都被丢弃
    if dropout == 1:
        return torch.zeros_like(X)
    # 在本情况中，所有元素都被保留
    if dropout == 0:
        return X
    mask = (torch.rand(X.shape) > dropout).float()
    return mask * X / (1.0 - dropout)
X= torch.arange(16, dtype = torch.float32).reshape((2, 8))
print(X)
print(dropout_layer(X, 0.))
print(dropout_layer(X, 0.5))
print(dropout_layer(X, 1.))
```

结果如下：

```
tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],
        [ 8.,  9., 10., 11., 12., 13., 14., 15.]])
        
tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],
        [ 8.,  9., 10., 11., 12., 13., 14., 15.]])
        
tensor([[ 0.,  2.,  0.,  6.,  8., 10.,  0.,  0.],
        [16.,  0.,  0., 22.,  0., 26.,  0.,  0.]])
        
tensor([[0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0.]])
```

####  1.2.3 调用Pytorch框架

对于深度学习框架的高级API，我们只需在每个全连接层之后添加一个`Dropout`层， 将暂退概率作为唯一的参数传递给它的构造函数。 在训练时，`Dropout`层将根据指定的暂退概率随机丢弃上一层的输出（相当于下一层的输入）。 在测试时，`Dropout`层仅传递数据。

> **Code Example**

```python
# dropout1，dropout2为概率
net = nn.Sequential(nn.Flatten(),
        nn.Linear(784, 256),
        nn.ReLU(),
        # 在第一个全连接层之后添加一个dropout层
        nn.Dropout(dropout1),
        nn.Linear(256, 256),
        nn.ReLU(),
        # 在第二个全连接层之后添加一个dropout层
        nn.Dropout(dropout2),
        nn.Linear(256, 10))

def init_weights(m):
    # 权重初始化
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, std=0.01)

net.apply(init_weights)
```

## 2、各类模型

### 2.1 经典CNN卷积神经网络

#### 2.1.1 互相关运算

严格来说，卷积层是个错误的叫法，因为它所表达的运算其实是**互相关运算**（cross-correlation），而不是卷积运算，但是为了方便起见，下面仍称之为卷积。

互相关运算如下：
$$
\begin{array}{}
\begin{array}{|c|c|c|}
	\hline 0&1&2\\
	\hline 3&4&5\\
	\hline 6&7&8\\
	\hline
\end{array} \quad * \quad 
\begin{array}{|c|c|}
	\hline 0&1\\
	\hline 2&3\\
	\hline
\end{array} \quad=\quad
\begin{array}{|c|c|}
	\hline 19&25\\
	\hline 37&43\\
	\hline
\end{array}
\tag{2.1}
\\
\\ 0*0+1*1+3*2+4*3=19
\end{array}
$$
假设二维被卷积的对象的尺寸为$m*n$，卷积核的尺寸为$h*w$，（行，列亦或为高，宽）我们可以得到互相关运算的结果尺寸如下：
$$
(m-h+1\quad,\quad n-w+1)
$$
当涉及`padding(填充)`和`stride(步长)`时可以得到更加普适的公式如下：

首先定义`padding`尺寸$(p_h\;,\; p_w)$，`stride`尺寸$(s_h\;,\;s_w)$，$s_h$表示行之间移动步长，$s_w$表示列之间移动的步长，可以得到卷积互相关运算的结果的尺寸公式如下（其中$[·]$表示向下取整）：
$$
([\frac{m+2*p_h-h}{s_h}]+1\quad ,\quad [\frac{n+2*p_w-w}{s_w}]+1)
$$
> **Calculation Example**

```python
X = torch.rand(size=(8, 8))
conv2d = nn.LazyConv2d(1, kernel_size=(3, 5), padding=(0, 1), stride=(3, 4))
comp_conv2d(conv2d, X).shape

# 得到结果如下 ((8-3+0)/3+1,(8-5+2*1)/4+1) 相除向下取整
torch.Size([2, 2])
```

> **Code Example**

```python
def corr2d(X, K):
    """Compute 2D cross-correlation."""
    h, w = K.shape
    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))
    for i in range(Y.shape[0]):
        for j in range(Y.shape[1]):
            Y[i, j] = (X[i:i + h, j:j + w] * K).sum()
    return Y
```

#### 2.1.2 多通道情况下的卷积（互相关运算）

##### (1) 多通道单输出

运算示意图：

![../_images/conv-multi-in.svg](https://zh.d2l.ai/_images/conv-multi-in.svg)

上述运算总结为将各通道的卷积值相加得到最终结果。

> **Code Example**

```python
def corr2d_multi_in(X, K):
    # X:(chanels,height,width)
    # Iterate through the 0th dimension (channel) of K first, then add them up
    return sum(corr2d(x, k) for x, k in zip(X, K))
```

##### (2) 多通道多输出

定义$c_i$，$c_o$为输入通道数和输出通道数，单个卷积核`kernel`的尺寸$(k_h\;,\;k_w)$，由多通道单输出的分析我们知道，由 $c_i \times k_h\times k_w$ 可以得到单输出的一个结果，如果想要获得 $c_o$ 个通道数的结果，那么我们便需要 $c_o$ 个 由Tensor序列 $c_i \times k_h\times k_w$ 构成的Tensor序列，即构造大小为 $c_o\times c_i \times k_h\times k_w$ 的`kernel`Tensor序列。

示意图：（1×1尺寸的kernel，输入通道为3，输出通道为2）

![../_images/conv-1x1.svg](https://d2l.ai/_images/conv-1x1.svg)

#### 2.1.3 池化（Pooling）

池化层常见的有最大池化和均值池化，池化可以看做为一种特殊的卷积，它也有`padding`和`stride`等参数，且依然遵守上面提到的卷积结果尺寸公式。

（碎碎念）有关池化的作用：降低卷积层对位置的敏感性，同时降低对空间降采样表示的敏感性。

#### 2.1.4 几个著名的CNN网络

##### (1) LeNet

* `LeNet`主要由两部分构成：卷积编码层和全连接层

数据流为手写数字数据集，在控制全连接层的模型复杂度时采用的策略是==weight decay==

![../_images/lenet.svg](https://zh.d2l.ai/_images/lenet.svg)

```python
LeNet = nn.Sequential(
    nn.Conv2d(1, 6, kernel_size=5, padding=2), nn.Sigmoid(),
    nn.AvgPool2d(kernel_size=2, stride=2),
    nn.Conv2d(6, 16, kernel_size=5), nn.Sigmoid(),
    nn.AvgPool2d(kernel_size=2, stride=2),
    nn.Flatten(),
    nn.Linear(16 * 5 * 5, 120), nn.Sigmoid(),
    nn.Linear(120, 84), nn.Sigmoid(),
    nn.Linear(84, 10))
```

##### (2) AlexNet

* `AlexNet`在控制全连接层的模型复杂度时采用的策略是==Drop out==

> 左边是LeNet，右边是AlexNet（略有改动）

![../_images/alexnet.svg](https://d2l.ai/_images/alexnet.svg)

```python
AlexNet = nn.Sequential(
            nn.LazyConv2d(96, kernel_size=11, stride=4, padding=1),
            nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2),
            nn.LazyConv2d(256, kernel_size=5, padding=2), nn.ReLU(),
            nn.MaxPool2d(kernel_size=3, stride=2),
            nn.LazyConv2d(384, kernel_size=3, padding=1), nn.ReLU(),
            nn.LazyConv2d(384, kernel_size=3, padding=1), nn.ReLU(),
            nn.LazyConv2d(256, kernel_size=3, padding=1), nn.ReLU(),
            nn.MaxPool2d(kernel_size=3, stride=2), nn.Flatten(),
            nn.LazyLinear(4096), nn.ReLU(), nn.Dropout(p=0.5),
            nn.LazyLinear(4096), nn.ReLU(),nn.Dropout(p=0.5),
            nn.LazyLinear(num_classes))
```

### 2.2 现代CNN

#### 2.2.1 使用块（Blocks）的网络VGG

我认为这种设计理念主要方便了现在训练好的网络模型与开发人员个人设计的网络的衔接，使得客制化神经网络更加容易，比如一个`hugging face`里的某些transformer模型权重已经训练好，个人有需求做一个NLP之类的任务，将训练好的模型直接拿过来作为一个Blocks，我们可以在其后面添加全连接层等之类作为另一个Blocks做一些简单的处理与训练即可满足需求。

此外，通过使用循环和子程序，可以很容易地在任何现代深度学习框架的代码中实现一些重复的架构。颇有封装的意味。

> 从AlexNet到VGG，主要区别为VGG由包含layers的块组成，AlexNet全部需要自主设计

![../_images/vgg.svg](https://d2l.ai/_images/vgg.svg)

> **Code Example**

```python
class VGG():
    def __init__(self, arch, lr=0.1, num_classes=10):
        self.lr = lr
        self.num_classes = num_classes
        conv_blks = []
        for (num_convs, out_channels) in arch:
            conv_blks.append(self.vgg_block(num_convs, out_channels))
        self.net = nn.Sequential(
            *conv_blks, nn.Flatten(),
            nn.LazyLinear(4096), nn.ReLU(), nn.Dropout(0.5),
            nn.LazyLinear(4096), nn.ReLU(), nn.Dropout(0.5),
            nn.LazyLinear(num_classes))
        # 权重初始化
        self.net.apply(init_cnn)
        
    def vgg_block(self, num_convs, out_channels):
        layers = []
        for _ in range(num_convs):
            layers.append(nn.LazyConv2d(out_channels, kernel_size=3, padding=1))
            layers.append(nn.ReLU())
        layers.append(nn.MaxPool2d(kernel_size=2,stride=2))
        return nn.Sequential(*layers)
```

#### 2.2.2 Network in Network（NiN）

AlexNet、VGG的工作主要都是为了继续拓展LeNet，继续拓宽加深网络结构，然而这造成了计算参数的大量增加

`《Dive into Deep Learning》`书中还提到了其不能提早添加全连接线性层来提高非线性度。

NiN主要改变在以下两点：

* 使用 $1\times1$ 的卷积核利用其激活函数来为模型增加非线性度（==nonlinearities==）
* 在最后的表征层添加全局的平均池化层来进行整合，需要注意的是如果没有添加非线性那么这层将不起作用

> 改进对象都是AlexNet，VGG和NiN的比较

![../_images/nin.svg](https://d2l.ai/_images/nin.svg)

> **Code Example**

```python
class NiN(d2l.Classifier):
    def __init__(self, lr=0.1, num_classes=10):
        self.lr = lr
        self.num_classes = num_classes
        self.net = nn.Sequential(
            self.nin_block(96, kernel_size=11, strides=4, padding=0),
            nn.MaxPool2d(3, stride=2),
            self.nin_block(256, kernel_size=5, strides=1, padding=2),
            nn.MaxPool2d(3, stride=2),
            self.nin_block(384, kernel_size=3, strides=1, padding=1),
            nn.MaxPool2d(3, stride=2),
            nn.Dropout(0.5),
            self.nin_block(num_classes, kernel_size=3, strides=1, padding=1),
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Flatten())
        self.net.apply(init_cnn)

    def nin_block(self, out_channels, kernel_size, strides, padding):
        return nn.Sequential(
            nn.LazyConv2d(out_channels, kernel_size, strides, padding), nn.ReLU(),
            # 后接两层1*1型卷积核层
            nn.LazyConv2d(out_channels, kernel_size=1), nn.ReLU(),
            nn.LazyConv2d(out_channels, kernel_size=1), nn.ReLU())
```

可以看到网络最后没有用到`fully connected layer`。

##### *  有关上面提到的网络的非线性度

从数学的角度来看，纯线性的组合
$$
\begin{aligned}
z^1 &= W^{1}x+b^1 \\
z^2 &= W^{2}z^1+b^2 \\ 
 &= W^2(W^1x+b^1)+b^2 \\
 &=W^2W^1x+(W^2b^1+b^2)\\

 \end{aligned}
$$

$$
\begin{flalign}
 令w = W^2W^1，b = W^2b^1+b^2，
 那么原方程z^2=wx+b显然仍是线性的
 即多层线性的组合等价于单线性层
 \end{flalign}
$$

非线性（Activation Function）可以增加模型的复杂度，使之更精确的逼近目标

那激活函数还有些什么用或者说类似于什么呢？

* 考虑神经网络的灵感，经过激活函数后的输出应该象征了神经元的活性，接近0说明不活跃，即激活函数在模拟人类神经的传递规则。
* 将输出限定在我们需要的范围内（==softmax==和==logistic==使之与概率联系起来）



