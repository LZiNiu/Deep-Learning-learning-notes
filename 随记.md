# 随记

## 1、模型中的一些操作

### 1.1 Softmax的数值计算问题

#### 1.1.1 典型的softmax代码

```python
def softmax(X):
    X_exp = torch.exp(X)
    partition = X_exp.sum(1, keepdims=True)
    return X_exp / partition
```

其虽然直观便于公式理解，但是对于数值不稳定没有足够的健壮性（robust），不建议在正式项目中这么写。



我们计算模型的输出并且使用了交叉熵损失。由于指数运算造成的数值向下溢出和向上溢出，这在数学上时完全合理的。通过softmax的函数表达形式$ \hat y_j = \frac{\exp o_j}{\sum_k \exp o_k} $，其中$k$表示有类别数，$j$则表示第$j$类，$o_j$表示模型在第$j$类上的输出，如果$o_j$很大且是正数，那么$exp(o_j)$将会是一个十分大的数造成数值的`overflow`，反之$o_j$很小（负数）那么将会造成数值的`underflow`，例如单精度浮点数大概范围为$10^{-38}$到$10^{38}$，因此，如果$o$的最大项位于区间$[-90,90]$外，那么计算结果将变得数值不稳定，一个解决办法如下：

定义：$\bar{o} = \stackrel{k}\max o_k$，上述softmax公式变换如下：
$$
\hat y_j = \frac{\exp o_j}{\sum_k \exp o_k} =
\frac{\exp(o_j - \bar{o}) \exp \bar{o}}{\sum_k \exp (o_k - \bar{o}) \exp \bar{o}} =
\frac{\exp(o_j - \bar{o})}{\sum_k \exp (o_k - \bar{o})}.
$$
显然，对于一个$q$分类问题，上式分母的数值必然被限定在区间$[1,q)$，而分子必定小于等于$1$，这解决了数值的overflow问题，但是，underflow问题依然存在，其出现时$exp(o_j-\bar o)$与$0$无异，在我们计算交叉熵$log\hat y_j$时将会导致计算$log0$，在反向传播时，我们将可能遇见许多NaN（Not a Number)的结果。

万幸的是，通过交叉熵和softmax二者结合能发现我们可以避开数值稳定性问题：

计算交叉熵损失时我们将会对softmax的结果进行取$log$的操作：
$$
\log \hat{y}_j =
\log \frac{\exp(o_j - \bar{o})}{\sum_k \exp (o_k - \bar{o})} =
o_j - \bar{o} - \log \sum_k \exp (o_k - \bar{o}).
$$

* 一点疑问：$log exp(o_j-\bar o)==o_j-\bar o???$



### 1.2 神经网络Dropout(暂退法)

克里斯托弗·毕晓普证明了具有输入噪声的训练等价于Tikhonov正则化。这项工作用数学证实了“要求函数光滑”和“要求函数对输入的随机噪声具有适应性”之间的联系。

* 经典泛化理论认为，为了缩小训练和测试性能之间的差距，应该以简单的模型为目标。参数的范数代表了一种有用的简单性度量。

* 另一个角度是平滑性，平滑性，即函数不应该对其输入的微小变化敏感。

在训练过程中，在计算后续层之前向网络的每一层注入噪声。 因为当训练一个有多层的深层网络时，注入噪声只会在输入-输出映射上增强平滑性。

#### 1.2.1 理论操作

`Dropout`在前向传播过程中，计算每一内部层的同时注入噪声。从表面上看是在训练过程中丢弃（drop out）一些神经元。 在整个训练过程的每一次迭代中，标准`Dropout`包括在计算下一层之前将当前层中的一些节点置零。

在标准`Dropout`正则化中，通过按保留（未丢弃）的节点的分数进行规范化来消除每一层的偏差。 换言之，每个中间活性值 $h$ 以暂退概率 $p$ 由随机变量 $h'$ 替换，如下所示：
$$
\begin{split}\begin{aligned}
h' =
\begin{cases}
    0 & \text{ 概率为 } p \\
    \frac{h}{1-p} & \text{ 其他情况}
\end{cases}
\end{aligned}\end{split}
$$
由以上公式易得$E(h')=h$，即期望仍保持不变。

#### 1.2.2 简单的单层Dropout代码实现

```python
def dropout_layer(X, dropout):
    assert 0 <= dropout <= 1
    # 在本情况中，所有元素都被丢弃
    if dropout == 1:
        return torch.zeros_like(X)
    # 在本情况中，所有元素都被保留
    if dropout == 0:
        return X
    mask = (torch.rand(X.shape) > dropout).float()
    return mask * X / (1.0 - dropout)
X= torch.arange(16, dtype = torch.float32).reshape((2, 8))
print(X)
print(dropout_layer(X, 0.))
print(dropout_layer(X, 0.5))
print(dropout_layer(X, 1.))
```

结果如下：

```
tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],
        [ 8.,  9., 10., 11., 12., 13., 14., 15.]])
        
tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],
        [ 8.,  9., 10., 11., 12., 13., 14., 15.]])
        
tensor([[ 0.,  2.,  0.,  6.,  8., 10.,  0.,  0.],
        [16.,  0.,  0., 22.,  0., 26.,  0.,  0.]])
        
tensor([[0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0.]])
```

####  1.2.3 调用Pytorch框架

对于深度学习框架的高级API，我们只需在每个全连接层之后添加一个`Dropout`层， 将暂退概率作为唯一的参数传递给它的构造函数。 在训练时，`Dropout`层将根据指定的暂退概率随机丢弃上一层的输出（相当于下一层的输入）。 在测试时，`Dropout`层仅传递数据。

**Example**

```python
# dropout1，dropout2为概率
net = nn.Sequential(nn.Flatten(),
        nn.Linear(784, 256),
        nn.ReLU(),
        # 在第一个全连接层之后添加一个dropout层
        nn.Dropout(dropout1),
        nn.Linear(256, 256),
        nn.ReLU(),
        # 在第二个全连接层之后添加一个dropout层
        nn.Dropout(dropout2),
        nn.Linear(256, 10))

def init_weights(m):
    # 权重初始化
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, std=0.01)

net.apply(init_weights)
```

## 2、各类模型

### 2.1 CNN卷积神经网络

#### 2.1.1 互相关运算

严格来说，卷积层是个错误的叫法，因为它所表达的运算其实是**互相关运算**（cross-correlation），而不是卷积运算，但是为了方便起见，下面仍称之为卷积。

互相关运算如下：
$$
\begin{array}{}
\begin{array}{|c|c|c|}
	\hline 0&1&2\\
	\hline 3&4&5\\
	\hline 6&7&8\\
	\hline
\end{array} \quad * \quad 
\begin{array}{|c|c|}
	\hline 0&1\\
	\hline 2&3\\
	\hline
\end{array} \quad=\quad
\begin{array}{|c|c|}
	\hline 19&25\\
	\hline 37&43\\
	\hline
\end{array}
\tag{2.1}
\\
\\ 0*0+1*1+3*2+4*3=19
\end{array}
$$
假设二维被卷积的对象的尺寸为$m*n$，卷积核的尺寸为$h*w$，（行，列亦或为高，宽）我们可以得到互相关运算的结果尺寸如下：
$$
(m-h+1\quad,\quad n-w+1)
$$
当涉及`padding(填充)`和`stride(步长)`时可以得到更加普适的公式如下：

首先定义`padding`尺寸$(p_h\;,\; p_w)$，`stride`尺寸$(s_h\;,\;s_w)$，$s_h$表示行之间移动步长，$s_w$表示列之间移动的步长，可以得到卷积互相关运算的结果的尺寸公式如下（其中$[·]$表示向下取整）：
$$
([\frac{m+2*p_h-h}{s_h}]+1\quad ,\quad [\frac{n+2*p_w-w}{s_w}]+1)
$$
**Calculation Example**

```python
X = torch.rand(size=(8, 8))
conv2d = nn.LazyConv2d(1, kernel_size=(3, 5), padding=(0, 1), stride=(3, 4))
comp_conv2d(conv2d, X).shape

# 得到结果如下 ((8-3+0)/3+1,(8-5+2*1)/4+1) 相除向下取整
torch.Size([2, 2])
```



**Code Example**

```python
def corr2d(X, K):
    """Compute 2D cross-correlation."""
    h, w = K.shape
    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))
    for i in range(Y.shape[0]):
        for j in range(Y.shape[1]):
            Y[i, j] = (X[i:i + h, j:j + w] * K).sum()
    return Y
```

#### 2.1.2 多通道情况下的卷积（互相关运算）

##### 2.1.2.1 多通道单输出

运算示意图：

![../_images/conv-multi-in.svg](https://zh.d2l.ai/_images/conv-multi-in.svg)

上述运算总结为将各通道的卷积值相加得到最终结果。

**Code Example**

```python
def corr2d_multi_in(X, K):
    # X:(chanels,height,width)
    # Iterate through the 0th dimension (channel) of K first, then add them up
    return sum(corr2d(x, k) for x, k in zip(X, K))
```

##### 2.1.2.2 多通道多输出

定义$c_i$，$c_o$为输入通道数和输出通道数，单个卷积核`kernel`的尺寸$(k_h\;,\;k_w)$，由多通道单输出的分析我们知道，由 $c_i \times k_h\times k_w$ 可以得到单输出的一个结果，如果想要获得 $c_o$ 个通道数的结果，那么我们便需要 $c_o$ 个 由Tensor序列 $c_i \times k_h\times k_w$ 构成的Tensor序列，即构造大小为 $c_o\times c_i \times k_h\times k_w$ 的`kernel`Tensor序列。

示意图：（1×1尺寸的kernel，输入通道为3，输出通道为2）

![../_images/conv-1x1.svg](https://d2l.ai/_images/conv-1x1.svg)

#### 2.1.3 池化（Pooling）

池化层常见的有最大池化和均值池化，池化可以看做为一种特殊的卷积，它也有`padding`和`stride`等参数，且依然遵守上面提到的卷积结果尺寸公式。

（碎碎念）有关池化的作用：







