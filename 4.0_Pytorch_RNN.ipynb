{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import time\n",
    "import seaborn\n",
    "from torch import nn\n",
    "from typing import Optional,Union,List\n",
    "from torchvision import transforms\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 文本读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('twinkled, and his usually pale face was flushed and animated. The\\n',\n",
       " 'The Time Machine, by H. G. Wells [1898]\\n\\n\\n\\n\\nI\\n\\n\\nThe Time Traveller (for so it will be convenient to ')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取语料\n",
    "# 注意不要写到同一次读取操作中,readlines或者read中任意一个都会占用文件流导致另外一个失效\n",
    "with open(\"timemachine.txt\", 'r') as file:\n",
    "    text_list = file.readlines()\n",
    "with open(\"timemachine.txt\", 'r') as file:\n",
    "    text_str = file.read()\n",
    "\n",
    "\n",
    "text_list[10],text_str[:100]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 文本预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('twinkled and his usually pale face was flushed and animated the',\n",
       " 'the time machine by h g wells i the time traveller for so it will be convenient to speak of him was ')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 利用正则表达式处理文本,将字母以外的标点符号替换为空格并都转换为小写\n",
    "text_list = [re.sub('[^A-Za-z]+',' ',line).strip().lower() for line in text_list]\n",
    "text_str = re.sub('[^A-Za-z]+',' ',text_str).strip().lower()\n",
    "text_list[10], text_str[:100]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构建词元"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(text:Union[List[str],str], token='word'):\n",
    "    # 以单词作为词元\n",
    "    if token=='word':\n",
    "        if type(text) is list:\n",
    "            return [line.split() for line in text]\n",
    "        else:\n",
    "            return text.split()\n",
    "    # 以字符作为词元\n",
    "    elif token=='char':\n",
    "        if type(text) is list:\n",
    "            return [list(line) for line in text]\n",
    "        else:\n",
    "            return list(text)\n",
    "    else:\n",
    "        raise TypeError(\"未知词元类型\")\n",
    "\n",
    "tokens_list_bychar = tokenize(text_list,'char')\n",
    "tokens_str_bychar = tokenize(text_str,'char')\n",
    "tokens_list_bychar[0]==tokens_str_bychar[:29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_list_byword = tokenize(text_list)\n",
    "tokens_str_byword = tokenize(text_str)\n",
    "tokens_list_byword[0]==tokens_str_byword[:7]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构建词汇表,词元和索引之间的映射关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(' ', 29927), ('e', 17838), ('t', 13515), ('a', 11704), ('i', 10138), ('n', 9917), ('o', 9758), ('s', 8486), ('h', 8257), ('r', 7674), ('d', 6337), ('l', 6146), ('m', 4043), ('u', 3805), ('c', 3424), ('f', 3354), ('w', 3225), ('g', 3075), ('y', 2679), ('p', 2427), ('b', 1897), ('v', 1295), ('k', 1087), ('x', 236), ('z', 144), ('j', 97), ('q', 95)]\n",
      "[' ', 'e', 't', 'a']\n",
      "[' ', 'a']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "class Vocab:\n",
    "    def __init__(self, tokens:List, min_freq=0, reserved_tokens:List=[]) -> None:\n",
    "        # 以字符作为词元时,tokens的形状应该为2维的list\n",
    "        # 将2维列表拉直\n",
    "        if tokens and isinstance(tokens[0],list):\n",
    "            tokens = [char for line in tokens for char in line]\n",
    "        # 计算每个char出现的频率\n",
    "        counter = Counter(tokens)\n",
    "        # 降序排列\n",
    "        self.token_freqs = sorted(counter.items(),key=lambda x:x[1], reverse=True)\n",
    "        # 制作词元索引\n",
    "        # 语料库中不存在或已删除的任何词元都将映射到一个特定的未知词元类  '<unk>(unknown token)'\n",
    "        self.index_to_token = reserved_tokens + ['<unk>']\n",
    "        # 词元token类映射到index\n",
    "        self.token_to_index = {token: idx for idx,token in enumerate(self.index_to_token)}\n",
    "\n",
    "        for token, freq in self.token_freqs:\n",
    "            if freq < min_freq:\n",
    "                break\n",
    "            if token not in self.token_to_index:\n",
    "                # 将词元类添加到词元表中\n",
    "                self.index_to_token.append(token)\n",
    "                # token类映射到index的dict对应的token的index设置为 len - 1\n",
    "                self.token_to_index[token] = len(self.index_to_token) - 1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.index_to_token)\n",
    "    \n",
    "    @property\n",
    "    def unk(self):\n",
    "        return self.token_to_index['<unk>']\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        # 重载[]运算符\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_index.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "    \n",
    "    def to_tokens(self, indices):\n",
    "        if not isinstance(indices,(list,tuple)) or isinstance(indices, slice):\n",
    "            return self.index_to_token[indices]\n",
    "        return [self.index_to_token[indice] for indice in indices]\n",
    "\n",
    "# 构建训练和测试数据格式\n",
    "def build(text, vocab=None, token='word', reserved_tokens=[]):\n",
    "    tokens = tokenize(text, token)\n",
    "    if vocab is None:\n",
    "        vocab = Vocab(tokens,reserved_tokens=reserved_tokens)\n",
    "    # 这里可以直接用tokens索引,但是当tokens是二维的时候需要额外拉直,故采用for创建\n",
    "    corpus = [vocab[tk] for tk in tokens]\n",
    "    return corpus, vocab\n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "\n",
    "# 测试词元为char\n",
    "temp = Vocab(tokens_list_bychar)\n",
    "print(temp.token_freqs)\n",
    "# temp = Vocab(tokens_str_bychar,reserved_tokens=['<pad>','<bos>'])\n",
    "# print(temp['<unk>','<pad>'])# getitem\n",
    "print(temp.to_tokens(slice(1,5)))\n",
    "print(temp.to_tokens([1,4]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'i', 'and', 'of'] 4580\n"
     ]
    }
   ],
   "source": [
    "# 测试word作为词元\n",
    "temp = Vocab(tokens_list_byword)\n",
    "print(temp.to_tokens(slice(1,5)), len(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 19, 50, 40, 2183, 2184, 400, 2, 1, 19]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试build函数功能\n",
    "X,_ = build(text_str)\n",
    "X[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ebd108c06cc1fee3274f6da06eb87bbf2b402824496318ac7157a648df7690c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
